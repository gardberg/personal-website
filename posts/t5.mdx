---
title: "ML Code is frustrating without shapes"
date: 2024-10-13
---

To finally make sense of attention and language models I decided to build a transformer model from scratch in Jax Numpy. I ended up doing Google's T5 model, which has an encoder-decoder architecture in contrast to the more popular decoder-only ones that most LLMs use. The whole process involved reading a lot of model code, mostly from huggingface. The realization i got from this was that...


#### Reading model code sucks without tensor shapes

Debugging model code is hard to do when the shapes of the tensors the functions act on are not known, and it is even harder if there is no type hinting (for example [here](https://github.com/huggingface/transformers/blob/e4ea19b958c89d61e42461fac6ac8441787121f8/src/transformers/models/t5/modeling_t5.py#L659)). A lot of functions i read in the huggingface codebase looked like this:

```python
def forward(
        self,
        hidden_states,
        ...
    ):
    
    # do stuff to `hidden_states`
    ...
    output = self.layer[0](
        hidden_states,
        ...
    )
```

I guess `hidden_states` is a tensor here. But is the batch dimension first, or second? Hard to figure out by just reading the code.

One solution here is to add comments detailing the dimensions:

```python
def forward(
        self,
        hidden_states,
        ...
    ):
    # hidden_states is here a jax ndarray of shape (batch_size, n_heads, seq_len, dim_per_head)
    
    # do stuff to `hidden_states`
    ...
    output = self.layer[0](
        hidden_states,
        ...
    )
```

This makes it easier. However, a better way is to actually combine the shape hinting with the type hinting. This can for example be done by creating a custom class which inherits from a generic class parametrized by a shape type:

```python
from jax.numpy import ndarray
from typing import Generic, TypeVar

class Array(ndarray, Generic[TypeVar("Shape")]):
    ...
```

This allows us to do:

```python
def forward(
        self,
        hidden_states: Array["batch_size, n_heads, seq_len, dim_per_head"],
        ...
    ):
    
    # do stuff to `hidden_states`
    ...
    output = self.layer[0](
        hidden_states,
        ...
    )
```

Which is nice! I'm no jax.numpy expert though, so there might be better ways of doing this. This worked for me!